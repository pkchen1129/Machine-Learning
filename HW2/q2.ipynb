{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python [conda env:GS_py37_tf_keras]","language":"python","name":"conda-env-GS_py37_tf_keras-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"q2.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"9mbIro0dXrvU","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Load data\n","q2_data = np.load('q2_data.npz')\n","q2x_train = q2_data['q2x_train']\n","q2y_train = q2_data['q2y_train']\n","q2x_test = q2_data['q2x_test']\n","q2y_test = q2_data['q2y_test']\n","# print(q2x_train.shape) #100 , 4\n","# print(q2y_train.shape) #100 , 1\n","# print(q2x_test.shape) #100 , 4\n","# print(q2y_test.shape) #100 , 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82FXoDFHXrxh","colab_type":"code","outputId":"27a6afa1-fe68-460f-b3ab-87565dcd2881","executionInfo":{"status":"ok","timestamp":1581398682139,"user_tz":300,"elapsed":468,"user":{"displayName":"Po-Kang Chen","photoUrl":"","userId":"03832949950970875412"}},"colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","\n","def softmax(z):\n","    return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n","\n","## Convert an iterable of indices to one-hot encoded labels.\n","def indices_to_one_hot(data, class_n):\n","    \n","    data.flatten()\n","    targets = list(data)\n","    for i in range (len(targets)):\n","      targets[i] = int(targets[i])\n","    # print(np.eye(class_n)[targets])\n","    return np.eye(class_n)[targets]\n","\n","alpha = 0.0005\n","iteration = 10\n","dim = 4\n","class_n = 3\n","w = np.zeros((dim, class_n))\n","X_test = q2x_test.T #for testing\n","\n","Y = indices_to_one_hot(q2y_train - np.ones_like(q2y_train) , class_n)\n","# Y_test = indices_to_one_hot(q2y_test - np.ones_like(q2y_test) , class_n)\n","\n","for j in range(iteration):\n","  grad = q2x_train.T @ (Y - softmax(q2x_train @ w))\n","  w = w - alpha * grad \n","\n","\n","\n","X = q2x_train\n","y = np.ravel(q2y_train)\n","clf = LogisticRegression(random_state=0).fit(X, y)\n","\n","X_T = q2x_test\n","y_T = np.ravel(q2y_test)\n","print('Train = ', clf.score(X , y))\n","print('Test = ', clf.score(X_T , y_T ))\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Train =  0.97\n","Test =  0.92\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]}]}